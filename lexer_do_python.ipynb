{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Lexer do Python\n",
    "\n",
    "Nesta aula iremos acessar o lexer do Python e reaproveitá-lo em outros projetos. O Python expõe tanto o seu lexer quanto o seu parser como bibliotecas incluídas na distribuição padrão. Iremos integrar o lexer do Python ao ox e, a partir daí, começar a utilizá-lo como lexer genérico em nossos projetos.\n",
    "\n",
    "Para começar, considere uma string de código Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "st = \"print('hello world!')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "A tokenização esta string deve gerar quatro tokens: **print**, **(**, **'hello world'** e **)**. Vamos importar o lexer do Python para gerar nossa função de tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Tokenizamos o código através da função `tokenize.tokenize`. Esta função possui uma interface curiosa. Ao invés de pedir uma string de código, ela requer uma função que, cada vez que for chamada, retorna uma nova linha do código como bytes. Vamos fazer uma função que recebe uma string de código e retorna uma função deste tipo\n",
    "\n",
    "Podemos ver a documentação com o comando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "tokenize.tokenize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Agora o nosso line getter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def make_line_getter(code):\n",
    "    lines = code.splitlines()\n",
    "    \n",
    "    def getter():\n",
    "        if lines:\n",
    "            # pop(0) retira e retorna o primeiro elemento da lista \n",
    "            line = lines.pop(0)\n",
    "            \n",
    "            # Agora convertemos para bytestring\n",
    "            return line.encode('utf-8')\n",
    "        \n",
    "        # Se não houverem mais linhas, retorna uma bytestring vazia\n",
    "        else:\n",
    "            return b''\n",
    "        \n",
    "    return getter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos imprimir todas tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenInfo(type=59 (ENCODING), string='utf-8', start=(0, 0), end=(0, 0), line='')\n",
      "TokenInfo(type=1 (NAME), string='print', start=(1, 0), end=(1, 5), line=\"print('hello world!')\")\n",
      "TokenInfo(type=53 (OP), string='(', start=(1, 5), end=(1, 6), line=\"print('hello world!')\")\n",
      "TokenInfo(type=3 (STRING), string=\"'hello world!'\", start=(1, 6), end=(1, 20), line=\"print('hello world!')\")\n",
      "TokenInfo(type=53 (OP), string=')', start=(1, 20), end=(1, 21), line=\"print('hello world!')\")\n",
      "TokenInfo(type=0 (ENDMARKER), string='', start=(2, 0), end=(2, 0), line='')\n"
     ]
    }
   ],
   "source": [
    "for tok in tokenize.tokenize(make_line_getter(st)):\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Observe que o tokenizer retorna objetos do tipo TokenInfo, enquanto o ox espera objetos \n",
    "do tipo ox.Token. Vamos fazer a conversão entre ambos. Existem algumas diferenças importantes entre as duas classes e é necessário fazer algumas conversões manuais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dicionário que guarda um mapa entre o código dos tokens e os\n",
    "# respectivos nomes.\n",
    "TYPE_CODE_TO_NAME = {}\n",
    "TYPE_NAME_TO_CODE = {}\n",
    "\n",
    "def to_ox_token(tok):\n",
    "    type_no = tok.type\n",
    "    value = tok.string\n",
    "    lineno = tok.start[0]\n",
    "    lexpos = tok.start[1]\n",
    "    type_str = TYPE_CODE_TO_NAME[type_no]\n",
    "    return Token(type_str, value, lineno, lexpos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "O nosso principal trabalho é determinar a conversão de código numérico usado pelo TokenInfo para uma string, como utiliza o ox.Token. Felizmente o módulo tokenize possui uma série de constantes que fazem estas conversões. Por exemplo, tokenize.NAME é uma constante com o valor 1, que corresponde ao código numérico para tokens do tipo NAME.\n",
    "\n",
    "Vamos utilizar a introspecção do Python para montar este dicionário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENDMARKER 0\n",
      "NAME 1\n",
      "NUMBER 2\n",
      "STRING 3\n",
      "NEWLINE 4\n",
      "INDENT 5\n",
      "DEDENT 6\n",
      "LPAR 7\n",
      "RPAR 8\n",
      "LSQB 9\n",
      "RSQB 10\n",
      "COLON 11\n",
      "COMMA 12\n",
      "SEMI 13\n",
      "PLUS 14\n",
      "MINUS 15\n",
      "STAR 16\n",
      "SLASH 17\n",
      "VBAR 18\n",
      "AMPER 19\n",
      "LESS 20\n",
      "GREATER 21\n",
      "EQUAL 22\n",
      "DOT 23\n",
      "PERCENT 24\n",
      "LBRACE 25\n",
      "RBRACE 26\n",
      "EQEQUAL 27\n",
      "NOTEQUAL 28\n",
      "LESSEQUAL 29\n",
      "GREATEREQUAL 30\n",
      "TILDE 31\n",
      "CIRCUMFLEX 32\n",
      "LEFTSHIFT 33\n",
      "RIGHTSHIFT 34\n",
      "DOUBLESTAR 35\n",
      "PLUSEQUAL 36\n",
      "MINEQUAL 37\n",
      "STAREQUAL 38\n",
      "SLASHEQUAL 39\n",
      "PERCENTEQUAL 40\n",
      "AMPEREQUAL 41\n",
      "VBAREQUAL 42\n",
      "CIRCUMFLEXEQUAL 43\n",
      "LEFTSHIFTEQUAL 44\n",
      "RIGHTSHIFTEQUAL 45\n",
      "DOUBLESTAREQUAL 46\n",
      "DOUBLESLASH 47\n",
      "DOUBLESLASHEQUAL 48\n",
      "AT 49\n",
      "ATEQUAL 50\n",
      "RARROW 51\n",
      "ELLIPSIS 52\n",
      "OP 53\n",
      "AWAIT 54\n",
      "ASYNC 55\n",
      "ERRORTOKEN 56\n",
      "N_TOKENS 60\n",
      "NT_OFFSET 256\n",
      "COMMENT 57\n",
      "NL 58\n",
      "ENCODING 59\n"
     ]
    }
   ],
   "source": [
    "namespace = vars(tokenize)  # converte o módulo para um dicionário\n",
    "\n",
    "for name, value in namespace.items():\n",
    "    # consideramos que as constantes de tipo possuem nomes em letras \n",
    "    # maiúsculas e valores numéricos\n",
    "    if name.isupper() and isinstance(value, int):\n",
    "        print(name, value)\n",
    "        TYPE_CODE_TO_NAME[value] = name\n",
    "        TYPE_NAME_TO_CODE[name] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos criar uma função que junta estas funcionalidades e gera tokens ox a partir do código Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def python_tokenize(source):\n",
    "    getter = make_line_getter(source)\n",
    "    tokens = tokenize.tokenize(getter)\n",
    "    ox_tokens = [to_ox_token(tok) for tok in tokens]\n",
    "    return ox_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ENCODING(utf-8),\n",
       " NAME(print),\n",
       " OP((),\n",
       " STRING('hello world!'),\n",
       " OP()),\n",
       " ENDMARKER()]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_tokenize(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que o Python cria 2 tokens \"virtuais\" que não existem originalmente no código: um token inicial de ENCODING e outro de ENDMARKER. Na realidade, o tokenizador do Python faz isto em outras situações: temos os tokens INDENT e DEDENT para fazer o controle de indentação e isto livra esta tarefa complicada do parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ENCODING(utf-8),\n",
       " NAME(while),\n",
       " NAME(True),\n",
       " OP(:),\n",
       " INDENT(    ),\n",
       " NAME(x),\n",
       " OP(+=),\n",
       " NUMBER(1),\n",
       " DEDENT(),\n",
       " ENDMARKER()]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = \"\"\"\n",
    "while True:\n",
    "    x += 1\n",
    "\"\"\"\n",
    "python_tokenize(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercício: Pytuguês\n",
    "\n",
    "Com o lexer do Python, podemos criar versões alternativas do Python quem simplesmente modificam algumas palavras chave. Um uso interessante para isto é realizar traduções do Python para outras linguagens\n",
    "\n",
    "Faça uma versão do \"Pytuguês\", traduzindo o Python para português. \n",
    "\n",
    "1. Tokenize com o tokenizador do Python\n",
    "2. Traduza alguns tokens especiais como (se -> if, else -> senão, while -> enquanto, etc).\n",
    "3. Gere o código Python a partir da lista de tokens modificada.\n",
    "4. Porque a abordagem por tokenização é melhor que uma simples substituição de strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
